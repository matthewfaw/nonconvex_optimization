{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from optimizers.perturbed_gd import PerturbedGD\n",
    "from optimizers.perturbed_agd import PerturbedAGD\n",
    "from optimizers.cubic_reg import StochasticCubicRegularizedNewton\n",
    "from models.nn import Net, SimpleNet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train dataset size: 60000\nTest dataset size: 10000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "grad_batch_size = 1\n",
    "hess_batch_size = 1\n",
    "\n",
    "dataset_train = MNIST('data/',\n",
    "                      train=True,\n",
    "                      download=True,\n",
    "                      transform=Compose([\n",
    "                            ToTensor(),\n",
    "                            Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "dataset_test = MNIST('data/',\n",
    "          train=False,\n",
    "          download=True,\n",
    "          transform=Compose([ToTensor(),\n",
    "                             Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "batch_size_test = len(dataset_test)\n",
    "\n",
    "print(\"Train dataset size:\",len(dataset_train))\n",
    "print(\"Test dataset size:\",len(dataset_test))\n",
    "train_loader_grad = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=grad_batch_size, shuffle=True)\n",
    "\n",
    "train_loader_hess = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=hess_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perturbed Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using defaults\n{   'chi': 126.68306919532642,\n    'd': 21840,\n    'eta': 0.01,\n    'f_thresh': 1.5554077913827313e-13,\n    'g_thresh': 6.2310731073563155e-09,\n    'r': 6.231073107356315e-13,\n    't_thresh': 4006.070396379457}\n",
      "At batch: 0 Test loss: tensor(2.3129, grad_fn=<NllLossBackward>) Train loss: tensor(2.5996, grad_fn=<NllLossBackward>)\n",
      "At batch: 1 Test loss: tensor(2.3076, grad_fn=<NllLossBackward>) Train loss: tensor(2.1930, grad_fn=<NllLossBackward>)\n",
      "At batch: 2 Test loss: tensor(2.3093, grad_fn=<NllLossBackward>) Train loss: tensor(2.1429, grad_fn=<NllLossBackward>)\n",
      "At batch: 3 Test loss: tensor(2.3092, grad_fn=<NllLossBackward>) Train loss: tensor(2.3448, grad_fn=<NllLossBackward>)\n",
      "At batch: 4 Test loss: tensor(2.3086, grad_fn=<NllLossBackward>) Train loss: tensor(2.4384, grad_fn=<NllLossBackward>)\n",
      "At batch: 5 Test loss: tensor(2.3085, grad_fn=<NllLossBackward>) Train loss: tensor(2.3086, grad_fn=<NllLossBackward>)\n",
      "At batch: 6 Test loss: tensor(2.3072, grad_fn=<NllLossBackward>) Train loss: tensor(2.1201, grad_fn=<NllLossBackward>)\n",
      "At batch: 7 Test loss: tensor(2.3070, grad_fn=<NllLossBackward>) Train loss: tensor(2.2721, grad_fn=<NllLossBackward>)\n",
      "At batch: 8 Test loss: tensor(2.3063, grad_fn=<NllLossBackward>) Train loss: tensor(2.3520, grad_fn=<NllLossBackward>)\n",
      "At batch: 9 Test loss: tensor(2.3058, grad_fn=<NllLossBackward>) Train loss: tensor(2.4202, grad_fn=<NllLossBackward>)\n",
      "At batch: 10 Test loss: tensor(2.3051, grad_fn=<NllLossBackward>) Train loss: tensor(2.5918, grad_fn=<NllLossBackward>)\n",
      "At batch: 11 Test loss: tensor(2.3035, grad_fn=<NllLossBackward>) Train loss: tensor(2.4293, grad_fn=<NllLossBackward>)\n",
      "At batch: 12 Test loss: tensor(2.3042, grad_fn=<NllLossBackward>) Train loss: tensor(2.3646, grad_fn=<NllLossBackward>)\n",
      "At batch: 13 Test loss: tensor(2.3037, grad_fn=<NllLossBackward>) Train loss: tensor(2.3655, grad_fn=<NllLossBackward>)\n",
      "At batch: 14 Test loss: tensor(2.3024, grad_fn=<NllLossBackward>) Train loss: tensor(2.2303, grad_fn=<NllLossBackward>)\n",
      "At batch: 15 Test loss: tensor(2.3033, grad_fn=<NllLossBackward>) Train loss: tensor(2.1800, grad_fn=<NllLossBackward>)\n",
      "At batch: 16 Test loss: tensor(2.3041, grad_fn=<NllLossBackward>) Train loss: tensor(2.3894, grad_fn=<NllLossBackward>)\n",
      "At batch: 17 Test loss: tensor(2.3046, grad_fn=<NllLossBackward>) Train loss: tensor(2.2189, grad_fn=<NllLossBackward>)\n",
      "At batch: 18 Test loss: tensor(2.3049, grad_fn=<NllLossBackward>) Train loss: tensor(2.6782, grad_fn=<NllLossBackward>)\n",
      "At batch: 19 Test loss: tensor(2.2989, grad_fn=<NllLossBackward>) Train loss: tensor(2.1347, grad_fn=<NllLossBackward>)\n",
      "At batch: 20 Test loss: tensor(2.3002, grad_fn=<NllLossBackward>) Train loss: tensor(2.3129, grad_fn=<NllLossBackward>)\n",
      "At batch: 21 Test loss: tensor(2.2995, grad_fn=<NllLossBackward>) Train loss: tensor(2.2730, grad_fn=<NllLossBackward>)\n",
      "At batch: 22 Test loss: tensor(2.3021, grad_fn=<NllLossBackward>) Train loss: tensor(2.4572, grad_fn=<NllLossBackward>)\n",
      "At batch: 23 Test loss: tensor(2.3020, grad_fn=<NllLossBackward>) Train loss: tensor(2.2993, grad_fn=<NllLossBackward>)\n",
      "At batch: 24 Test loss: tensor(2.3032, grad_fn=<NllLossBackward>) Train loss: tensor(2.1724, grad_fn=<NllLossBackward>)\n",
      "At batch: 25 Test loss: tensor(2.3031, grad_fn=<NllLossBackward>) Train loss: tensor(2.4717, grad_fn=<NllLossBackward>)\n",
      "At batch: 26 Test loss: tensor(2.3035, grad_fn=<NllLossBackward>) Train loss: tensor(2.1979, grad_fn=<NllLossBackward>)\n",
      "At batch: 27 Test loss: tensor(2.3043, grad_fn=<NllLossBackward>) Train loss: tensor(2.5180, grad_fn=<NllLossBackward>)\n",
      "At batch: 28 Test loss: tensor(2.2991, grad_fn=<NllLossBackward>) Train loss: tensor(2.2985, grad_fn=<NllLossBackward>)\n",
      "At batch: 29 Test loss: tensor(2.3002, grad_fn=<NllLossBackward>) Train loss: tensor(2.5989, grad_fn=<NllLossBackward>)\n",
      "At batch: 30 Test loss: tensor(2.2995, grad_fn=<NllLossBackward>) Train loss: tensor(2.4736, grad_fn=<NllLossBackward>)\n",
      "At batch: 31 Test loss: tensor(2.2996, grad_fn=<NllLossBackward>) Train loss: tensor(2.4009, grad_fn=<NllLossBackward>)\n",
      "At batch: 32 Test loss: tensor(2.2968, grad_fn=<NllLossBackward>) Train loss: tensor(2.2460, grad_fn=<NllLossBackward>)\n",
      "At batch: 33 Test loss: tensor(2.2961, grad_fn=<NllLossBackward>) Train loss: tensor(2.2373, grad_fn=<NllLossBackward>)\n",
      "At batch: 34 Test loss: tensor(2.2967, grad_fn=<NllLossBackward>) Train loss: tensor(2.3231, grad_fn=<NllLossBackward>)\n",
      "At batch: 35 Test loss: tensor(2.2952, grad_fn=<NllLossBackward>) Train loss: tensor(2.2602, grad_fn=<NllLossBackward>)\n",
      "At batch: 36 Test loss: tensor(2.2941, grad_fn=<NllLossBackward>) Train loss: tensor(2.2348, grad_fn=<NllLossBackward>)\n",
      "At batch: 37 Test loss: tensor(2.2949, grad_fn=<NllLossBackward>) Train loss: tensor(2.3450, grad_fn=<NllLossBackward>)\n",
      "At batch: 38 Test loss: tensor(2.2927, grad_fn=<NllLossBackward>) Train loss: tensor(2.4049, grad_fn=<NllLossBackward>)\n",
      "At batch: 39 Test loss: tensor(2.2924, grad_fn=<NllLossBackward>) Train loss: tensor(2.3688, grad_fn=<NllLossBackward>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "model_pgd = Net()\n",
    "loss_fn_pgd = F.nll_loss\n",
    "\n",
    "d = np.sum([p.numel() for p in Net().parameters()])\n",
    "\n",
    "optimizer_pgd = PerturbedGD(model_pgd.parameters(),\n",
    "                        l=10000,\n",
    "                        rho=100,\n",
    "                        epsilon=1e-5,\n",
    "                        c=100,\n",
    "                        delta=.1,\n",
    "                        delta_f=10,\n",
    "                        d=d)\n",
    "iter_grad = iter(train_loader_grad)\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "batch_idx = 0\n",
    "losses_pgd_test = []\n",
    "losses_pgd_train = []\n",
    "while True:\n",
    "    if batch_idx == 40:\n",
    "        break\n",
    "        \n",
    "    nextloss = loss_fn_pgd(model_pgd(test_data), test_labels)\n",
    "    losses_pgd_test.append(nextloss)\n",
    "    if optimizer_pgd._is_done:\n",
    "        print(\"Optimizer has hit early stopping condition\")\n",
    "        break\n",
    "    data_for_grad = next(iter_grad, None)\n",
    "    if data_for_grad is None: \n",
    "        print(\"Exhausted training data -- finished optimization\")\n",
    "        break\n",
    "    features_g_pgd, labels_g_pgd = data_for_grad\n",
    "    def closure():\n",
    "        optimizer_pgd.zero_grad()\n",
    "        loss_g_pgd = loss_fn_pgd(model_pgd(features_g_pgd), labels_g_pgd)\n",
    "        loss_g_pgd.backward()\n",
    "        return loss_g_pgd\n",
    "    \n",
    "    train_loss = optimizer_pgd.step(closure)\n",
    "    losses_pgd_train.append(train_loss)\n",
    "    \n",
    "    print(\"At batch:\",batch_idx, \"Test loss:\",nextloss, \"Train loss:\",train_loss)\n",
    "    batch_idx += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perturbed Accelerated Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using defaults\n{   'T': 4000,\n    'add_noise': True,\n    'epsilon': 1e-05,\n    'eta': 0.01,\n    'gamma': 0.1,\n    'neg_curv_explore': True,\n    'r': 1e-05,\n    's': 100000.0,\n    'theta': 0.1}\n",
      "Exploring neg curv\nAt batch: 0 test loss: tensor(2.3121, grad_fn=<NllLossBackward>) train loss: tensor(2.2442, grad_fn=<NllLossBackward>)\n",
      "Exploring neg curv\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bbe5635413a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_g_apgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_apgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mlosses_apgd_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/grad_school/year1/lso_II/final_project/optimizers/perturbed_agd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneg_curv_explore\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloss_at_start\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss_at_yt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_at_y1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t_flat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t_flat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exploring neg curv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mx_t_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_negative_curvature_exploration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t_plus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ],
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "model_apgd = Net()\n",
    "loss_fn_apgd = F.nll_loss\n",
    "\n",
    "d = np.sum([p.numel() for p in Net().parameters()])\n",
    "\n",
    "optimizer_apgd = PerturbedAGD(model_apgd.parameters(),\n",
    "                        eta=0.01,\n",
    "                        theta=0.1,\n",
    "                        gamma=0.1,\n",
    "                        s=1e5,\n",
    "                        r=1e-5,\n",
    "                        T=4000,\n",
    "                        epsilon=1e-5,\n",
    "                        add_noise=True,\n",
    "                        neg_curv_explore=True)\n",
    "iter_grad = iter(train_loader_grad)\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "batch_idx = 0\n",
    "losses_apgd_test = []\n",
    "losses_apgd_train = []\n",
    "while True:\n",
    "    if batch_idx == 40:\n",
    "        break\n",
    "        \n",
    "    testloss = loss_fn_apgd(model_apgd(test_data), test_labels)\n",
    "    losses_apgd_test.append(testloss)\n",
    "    if optimizer_apgd._is_done:\n",
    "        print(\"Optimizer has hit early stopping condition\")\n",
    "        break\n",
    "    data_for_grad = next(iter_grad, None)\n",
    "    if data_for_grad is None: \n",
    "        print(\"Exhausted training data -- finished optimization\")\n",
    "        break\n",
    "    features_g_apgd, labels_g_apgd = data_for_grad\n",
    "    def closure():\n",
    "        optimizer_apgd.zero_grad()\n",
    "        loss_g_apgd = loss_fn_apgd(model_apgd(features_g_apgd), labels_g_apgd)\n",
    "        loss_g_apgd.backward()\n",
    "        return loss_g_apgd\n",
    "    \n",
    "    train_loss = optimizer_apgd.step(closure)\n",
    "    losses_apgd_train.append(train_loss)\n",
    "    \n",
    "    print(\"At batch:\",batch_idx, \"test loss:\", testloss, \"train loss:\", train_loss)\n",
    "    batch_idx += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Cubic Regularized Newton's method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model_scrn = Net()\n",
    "loss_fn_scrn = F.nll_loss\n",
    "\n",
    "optimizer = StochasticCubicRegularizedNewton(model_scrn.parameters(),\n",
    "                                             l=100,\n",
    "                                             rho=100,\n",
    "                                             epsilon=1e-4,\n",
    "                                             c_prime=1)\n",
    "\n",
    "\n",
    "iter_grad = iter(train_loader_grad)\n",
    "iter_hess = iter(train_loader_hess)\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "batch_idx = 0\n",
    "losses_scrn_test = []\n",
    "losses_scrn_train = []\n",
    "while True:\n",
    "    testloss = loss_fn_scrn(model_scrn(test_data), test_labels)\n",
    "    losses_scrn_test.append(testloss)\n",
    "    if optimizer._is_done:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    data_for_grad = next(iter_grad, None)\n",
    "    data_for_hess = next(iter_hess, None)\n",
    "    \n",
    "    if data_for_grad is None or data_for_hess is None:\n",
    "        print(\"Exhausted training data -- finished optimization\")\n",
    "        break\n",
    "    features_g, labels_g = data_for_grad\n",
    "    features_h, labels_h = data_for_hess\n",
    "    \n",
    "    loss_h = loss_fn_scrn(model_scrn(features_h), labels_h)\n",
    "    \n",
    "    flattened_grad_h = []\n",
    "    for p in model_scrn.parameters():\n",
    "        flattened_grad_h.append(autograd.grad(loss_h, p, create_graph=True)[0].view(-1))\n",
    "    flattened_grad_h = torch.cat(flattened_grad_h)\n",
    "    \n",
    "    loss_g = loss_fn_scrn(model_scrn(features_g), labels_g)\n",
    "    loss_g.backward()\n",
    "    \n",
    "    optimizer.step(flattened_grad_h)\n",
    "    losses_scrn_train.append(loss_g)\n",
    "    \n",
    "    print(\"At batch:\",batch_idx,\"Test loss:\",testloss, \"Train loss:\",loss_g)\n",
    "    batch_idx += 1\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(range(len(losses_scrn_test)), losses_scrn_test, 'ks-', label='StochasticCubic')\n",
    "plt.plot(range(len(losses_pgd_test)), losses_pgd_test, 'b^-', label='PertGD')\n",
    "plt.plot(range(len(losses_apgd_test)), losses_apgd_test, 'ro-', label='PertAGD')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Outer loop iteration number\")\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"Batch size for gradient=%d and for hessian=%d\"%(grad_batch_size, hess_batch_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(range(len(losses_scrn_train)), losses_scrn_train, 'ks-', label='StochasticCubic')\n",
    "plt.plot(range(len(losses_pgd_train)), losses_pgd_train, 'b^-', label='PertGD')\n",
    "plt.plot(range(len(losses_apgd_train)), losses_apgd_train, 'ro-', label='PertAGD')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Outer loop iteration number\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Batch size for gradient=%d and for hessian=%d\"%(grad_batch_size, hess_batch_size))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}